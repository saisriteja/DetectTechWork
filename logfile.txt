2022-07-01 16:19:16,674 [INFO] numba.transforms: finding looplift candidates

__________________________________________________________________________________________________
block_3a_bn_1 (BatchNormalizati (None, 256, 14, 14)  1024        block_3a_conv_1[0][0]
__________________________________________________________________________________________________
block_3a_relu_1 (Activation)    (None, 256, 14, 14)  0           block_3a_bn_1[0][0]
__________________________________________________________________________________________________
block_3a_conv_2 (Conv2D)        (None, 256, 14, 14)  589824      block_3a_relu_1[0][0]
__________________________________________________________________________________________________
block_3a_conv_shortcut (Conv2D) (None, 256, 14, 14)  32768       block_2b_relu[0][0]
__________________________________________________________________________________________________
block_3a_bn_2 (BatchNormalizati (None, 256, 14, 14)  1024        block_3a_conv_2[0][0]
__________________________________________________________________________________________________
block_3a_bn_shortcut (BatchNorm (None, 256, 14, 14)  1024        block_3a_conv_shortcut[0][0]
__________________________________________________________________________________________________
add_5 (Add)                     (None, 256, 14, 14)  0           block_3a_bn_2[0][0]
                                                                 block_3a_bn_shortcut[0][0]
__________________________________________________________________________________________________
block_3a_relu (Activation)      (None, 256, 14, 14)  0           add_5[0][0]
__________________________________________________________________________________________________
block_3b_conv_1 (Conv2D)        (None, 256, 14, 14)  589824      block_3a_relu[0][0]
__________________________________________________________________________________________________
block_3b_bn_1 (BatchNormalizati (None, 256, 14, 14)  1024        block_3b_conv_1[0][0]
__________________________________________________________________________________________________
block_3b_relu_1 (Activation)    (None, 256, 14, 14)  0           block_3b_bn_1[0][0]
__________________________________________________________________________________________________
block_3b_conv_2 (Conv2D)        (None, 256, 14, 14)  589824      block_3b_relu_1[0][0]
__________________________________________________________________________________________________
block_3b_bn_2 (BatchNormalizati (None, 256, 14, 14)  1024        block_3b_conv_2[0][0]
__________________________________________________________________________________________________
add_6 (Add)                     (None, 256, 14, 14)  0           block_3b_bn_2[0][0]
                                                                 block_3a_relu[0][0]
__________________________________________________________________________________________________
block_3b_relu (Activation)      (None, 256, 14, 14)  0           add_6[0][0]
__________________________________________________________________________________________________
block_4a_conv_1 (Conv2D)        (None, 512, 7, 7)    1179648     block_3b_relu[0][0]
__________________________________________________________________________________________________
block_4a_bn_1 (BatchNormalizati (None, 512, 7, 7)    2048        block_4a_conv_1[0][0]
__________________________________________________________________________________________________
block_4a_relu_1 (Activation)    (None, 512, 7, 7)    0           block_4a_bn_1[0][0]
__________________________________________________________________________________________________
block_4a_conv_2 (Conv2D)        (None, 512, 7, 7)    2359296     block_4a_relu_1[0][0]
__________________________________________________________________________________________________
block_4a_conv_shortcut (Conv2D) (None, 512, 7, 7)    131072      block_3b_relu[0][0]
__________________________________________________________________________________________________
block_4a_bn_2 (BatchNormalizati (None, 512, 7, 7)    2048        block_4a_conv_2[0][0]
__________________________________________________________________________________________________
block_4a_bn_shortcut (BatchNorm (None, 512, 7, 7)    2048        block_4a_conv_shortcut[0][0]
__________________________________________________________________________________________________
add_7 (Add)                     (None, 512, 7, 7)    0           block_4a_bn_2[0][0]
                                                                 block_4a_bn_shortcut[0][0]
__________________________________________________________________________________________________
block_4a_relu (Activation)      (None, 512, 7, 7)    0           add_7[0][0]
__________________________________________________________________________________________________
block_4b_conv_1 (Conv2D)        (None, 512, 7, 7)    2359296     block_4a_relu[0][0]
__________________________________________________________________________________________________
block_4b_bn_1 (BatchNormalizati (None, 512, 7, 7)    2048        block_4b_conv_1[0][0]
__________________________________________________________________________________________________
block_4b_relu_1 (Activation)    (None, 512, 7, 7)    0           block_4b_bn_1[0][0]
__________________________________________________________________________________________________
block_4b_conv_2 (Conv2D)        (None, 512, 7, 7)    2359296     block_4b_relu_1[0][0]
__________________________________________________________________________________________________
block_4b_bn_2 (BatchNormalizati (None, 512, 7, 7)    2048        block_4b_conv_2[0][0]
__________________________________________________________________________________________________
add_8 (Add)                     (None, 512, 7, 7)    0           block_4b_bn_2[0][0]
                                                                 block_4a_relu[0][0]
__________________________________________________________________________________________________
block_4b_relu (Activation)      (None, 512, 7, 7)    0           add_8[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 512, 1, 1)    0           block_4b_relu[0][0]
__________________________________________________________________________________________________
avg_pool (AveragePooling2D)     (None, 512, 1, 1)    0           average_pooling2d_1[0][0]
__________________________________________________________________________________________________
flatten (Flatten)               (None, 512)          0           avg_pool[0][0]
__________________________________________________________________________________________________
predictions (Dense)             (None, 2)            1026        flatten[0][0]
==================================================================================================
Total params: 11,191,490
Trainable params: 11,181,762
Non-trainable params: 9,728
__________________________________________________________________________________________________
Epoch 1/10

 1/60 [..............................] - ETA: 4:51 - loss: 0.8640 - acc: 0.3000/usr/local/lib/python3.6/dist-packages/numba/object_mode_passes.py:178: NumbaWarning: Function "random_contrast" was compiled in object mode without forceobj=True.

File "../root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/makenet/utils/helper.py", line 174:
<source missing, REPL/exec in use?>

  state.func_ir.loc))
/usr/local/lib/python3.6/dist-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning:
Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.

For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit

File "../root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/makenet/utils/helper.py", line 174:
<source missing, REPL/exec in use?>

  state.func_ir.loc))
WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/utils.py:186: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.

2022-07-01 16:19:21,610 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/utils.py:186: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.


 2/60 [>.............................] - ETA: 2:51 - loss: 1.0372 - acc: 0.3500
 5/60 [=>............................] - ETA: 1:05 - loss: 0.9367 - acc: 0.3800
 8/60 [===>..........................] - ETA: 39s - loss: 0.8707 - acc: 0.4250
11/60 [====>.........................] - ETA: 27s - loss: 0.8846 - acc: 0.4545
14/60 [======>.......................] - ETA: 20s - loss: 0.8604 - acc: 0.5143
17/60 [=======>......................] - ETA: 15s - loss: 0.8633 - acc: 0.4941
19/60 [========>.....................] - ETA: 13s - loss: 0.8439 - acc: 0.5158
21/60 [=========>....................] - ETA: 11s - loss: 0.8498 - acc: 0.5286
23/60 [==========>...................] - ETA: 10s - loss: 0.8255 - acc: 0.5391
26/60 [============>.................] - ETA: 8s - loss: 0.8200 - acc: 0.5385
29/60 [=============>................] - ETA: 7s - loss: 0.7924 - acc: 0.5586
31/60 [==============>...............] - ETA: 6s - loss: 0.7757 - acc: 0.5710
33/60 [===============>..............] - ETA: 5s - loss: 0.7625 - acc: 0.5848
35/60 [================>.............] - ETA: 4s - loss: 0.7795 - acc: 0.5857
38/60 [==================>...........] - ETA: 4s - loss: 0.8322 - acc: 0.5711
39/60 [==================>...........] - ETA: 3s - loss: 0.8268 - acc: 0.5744
40/60 [===================>..........] - ETA: 3s - loss: 0.8244 - acc: 0.5750
42/60 [====================>.........] - ETA: 3s - loss: 0.8510 - acc: 0.5643
45/60 [=====================>........] - ETA: 2s - loss: 0.8392 - acc: 0.5667
48/60 [=======================>......] - ETA: 1s - loss: 0.8414 - acc: 0.5688
50/60 [========================>.....] - ETA: 1s - loss: 0.8331 - acc: 0.5720
53/60 [=========================>....] - ETA: 1s - loss: 0.8348 - acc: 0.5623
54/60 [==========================>...] - ETA: 0s - loss: 0.8301 - acc: 0.5667
57/60 [===========================>..] - ETA: 0s - loss: 0.8182 - acc: 0.5719
58/60 [============================>.] - ETA: 0s - loss: 0.8139 - acc: 0.5759
60/60 [==============================] - 9s 157ms/step - loss: 0.8182 - acc: 0.5700 - val_loss: 0.9125 - val_acc: 0.5948283ae3f8ffbc:135:190 [0] NCCL INFO Bootstrap : Using eth0:172.17.0.2<0>
283ae3f8ffbc:135:190 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
283ae3f8ffbc:135:190 [0] NCCL INFO P2P plugin IBext
283ae3f8ffbc:135:190 [0] NCCL INFO NET/IB : No device found.
283ae3f8ffbc:135:190 [0] NCCL INFO NET/IB : No device found.
283ae3f8ffbc:135:190 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
283ae3f8ffbc:135:190 [0] NCCL INFO Using network Socket
NCCL version 2.11.4+cuda11.6
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 00/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 01/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 02/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 03/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 04/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 05/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 06/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 07/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 08/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 09/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 10/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 11/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 12/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 13/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 14/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 15/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 16/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 17/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 18/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 19/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 20/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 21/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 22/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 23/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 24/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 25/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 26/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 27/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 28/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 29/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 30/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Channel 31/32 :    0
283ae3f8ffbc:135:190 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
283ae3f8ffbc:135:190 [0] NCCL INFO Connected all rings
283ae3f8ffbc:135:190 [0] NCCL INFO Connected all trees
283ae3f8ffbc:135:190 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
283ae3f8ffbc:135:190 [0] NCCL INFO comm 0x7f61ec7d21e0 rank 0 nranks 1 cudaDev 0 busId 6000 - Init COMPLETE
/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.482479). Check your callbacks.
  % delta_t_median)
2022-07-01 21:07:34,388 [INFO] root: Training loop in progress
Epoch 2/10

 1/60 [..............................] - ETA: 1s - loss: 0.8355 - acc: 0.4000
 3/60 [>.............................] - ETA: 1s - loss: 0.6932 - acc: 0.6000
 5/60 [=>............................] - ETA: 1s - loss: 0.8451 - acc: 0.5800
 7/60 [==>...........................] - ETA: 1s - loss: 0.8114 - acc: 0.5857
 9/60 [===>..........................] - ETA: 1s - loss: 0.7760 - acc: 0.6000
12/60 [=====>........................] - ETA: 1s - loss: 0.7697 - acc: 0.5833
15/60 [======>.......................] - ETA: 1s - loss: 0.7570 - acc: 0.5667
18/60 [========>.....................] - ETA: 1s - loss: 0.7313 - acc: 0.5833
19/60 [========>.....................] - ETA: 1s - loss: 0.7242 - acc: 0.5895
22/60 [==========>...................] - ETA: 1s - loss: 0.7127 - acc: 0.6045
24/60 [===========>..................] - ETA: 1s - loss: 0.6999 - acc: 0.6208
27/60 [============>.................] - ETA: 0s - loss: 0.6854 - acc: 0.6259
30/60 [==============>...............] - ETA: 0s - loss: 0.6825 - acc: 0.6267
31/60 [==============>...............] - ETA: 0s - loss: 0.6771 - acc: 0.6323
34/60 [================>.............] - ETA: 0s - loss: 0.6642 - acc: 0.6441
35/60 [================>.............] - ETA: 0s - loss: 0.6595 - acc: 0.6457
37/60 [=================>............] - ETA: 0s - loss: 0.6599 - acc: 0.6459
39/60 [==================>...........] - ETA: 0s - loss: 0.6662 - acc: 0.6436
41/60 [===================>..........] - ETA: 0s - loss: 0.6628 - acc: 0.6488
44/60 [=====================>........] - ETA: 0s - loss: 0.6575 - acc: 0.6545
46/60 [======================>.......] - ETA: 0s - loss: 0.6601 - acc: 0.6522
47/60 [======================>.......] - ETA: 0s - loss: 0.6569 - acc: 0.6553
48/60 [=======================>......] - ETA: 0s - loss: 0.6535 - acc: 0.6583
51/60 [========================>.....] - ETA: 0s - loss: 0.6550 - acc: 0.6588
54/60 [==========================>...] - ETA: 0s - loss: 0.6643 - acc: 0.6556
56/60 [===========================>..] - ETA: 0s - loss: 0.6671 - acc: 0.6554
58/60 [============================>.] - ETA: 0s - loss: 0.6614 - acc: 0.6552
60/60 [==============================] - 3s 53ms/step - loss: 0.6570 - acc: 0.6600 - val_loss: 0.4542 - val_acc: 0.8072
2022-07-01 21:07:39,899 [INFO] root: Training loop in progress
Epoch 3/10

 1/60 [..............................] - ETA: 1s - loss: 0.5599 - acc: 0.7000
 3/60 [>.............................] - ETA: 1s - loss: 0.5651 - acc: 0.7333
 5/60 [=>............................] - ETA: 1s - loss: 0.5076 - acc: 0.7600
 7/60 [==>...........................] - ETA: 1s - loss: 0.5154 - acc: 0.7429
10/60 [====>.........................] - ETA: 1s - loss: 0.5540 - acc: 0.7300
13/60 [=====>........................] - ETA: 1s - loss: 0.5539 - acc: 0.7231
15/60 [======>.......................] - ETA: 1s - loss: 0.5275 - acc: 0.7467
18/60 [========>.....................] - ETA: 1s - loss: 0.5179 - acc: 0.7667
20/60 [=========>....................] - ETA: 1s - loss: 0.5810 - acc: 0.7350
23/60 [==========>...................] - ETA: 0s - loss: 0.5810 - acc: 0.7261
25/60 [===========>..................] - ETA: 0s - loss: 0.5762 - acc: 0.7200
27/60 [============>.................] - ETA: 0s - loss: 0.5769 - acc: 0.7222
29/60 [=============>................] - ETA: 0s - loss: 0.5721 - acc: 0.7276
31/60 [==============>...............] - ETA: 0s - loss: 0.5646 - acc: 0.7387
33/60 [===============>..............] - ETA: 0s - loss: 0.5702 - acc: 0.7333
35/60 [================>.............] - ETA: 0s - loss: 0.5631 - acc: 0.7400
37/60 [=================>............] - ETA: 0s - loss: 0.5638 - acc: 0.7405
38/60 [==================>...........] - ETA: 0s - loss: 0.5744 - acc: 0.7395
41/60 [===================>..........] - ETA: 0s - loss: 0.5754 - acc: 0.7366
44/60 [=====================>........] - ETA: 0s - loss: 0.5630 - acc: 0.7455
47/60 [======================>.......] - ETA: 0s - loss: 0.5545 - acc: 0.7447
50/60 [========================>.....] - ETA: 0s - loss: 0.5573 - acc: 0.7480
52/60 [=========================>....] - ETA: 0s - loss: 0.5553 - acc: 0.7481
55/60 [==========================>...] - ETA: 0s - loss: 0.5566 - acc: 0.7473
56/60 [===========================>..] - ETA: 0s - loss: 0.5627 - acc: 0.7429
59/60 [============================>.] - ETA: 0s - loss: 0.5585 - acc: 0.7424
60/60 [==============================] - 3s 54ms/step - loss: 0.5611 - acc: 0.7383 - val_loss: 0.3595 - val_acc: 0.8791
2022-07-01 21:07:44,291 [INFO] root: Training loop in progress
Epoch 4/10

 1/60 [..............................] - ETA: 1s - loss: 0.5540 - acc: 0.5000
 3/60 [>.............................] - ETA: 1s - loss: 0.5376 - acc: 0.7000
 5/60 [=>............................] - ETA: 1s - loss: 0.5967 - acc: 0.7400
 7/60 [==>...........................] - ETA: 1s - loss: 0.5188 - acc: 0.7857
 9/60 [===>..........................] - ETA: 1s - loss: 0.5297 - acc: 0.7889
12/60 [=====>........................] - ETA: 1s - loss: 0.5392 - acc: 0.8000
14/60 [======>.......................] - ETA: 1s - loss: 0.5449 - acc: 0.8071
17/60 [=======>......................] - ETA: 1s - loss: 0.5451 - acc: 0.7882
19/60 [========>.....................] - ETA: 1s - loss: 0.5392 - acc: 0.7842
22/60 [==========>...................] - ETA: 1s - loss: 0.5484 - acc: 0.7727
24/60 [===========>..................] - ETA: 1s - loss: 0.5497 - acc: 0.7583
26/60 [============>.................] - ETA: 1s - loss: 0.5640 - acc: 0.7500
27/60 [============>.................] - ETA: 1s - loss: 0.5609 - acc: 0.7556
30/60 [==============>...............] - ETA: 1s - loss: 0.5475 - acc: 0.7667
33/60 [===============>..............] - ETA: 0s - loss: 0.5418 - acc: 0.7758
35/60 [================>.............] - ETA: 0s - loss: 0.5323 - acc: 0.7829
36/60 [=================>............] - ETA: 1s - loss: 0.5316 - acc: 0.7861
39/60 [==================>...........] - ETA: 0s - loss: 0.5340 - acc: 0.7872
42/60 [====================>.........] - ETA: 0s - loss: 0.5463 - acc: 0.7833
45/60 [=====================>........] - ETA: 0s - loss: 0.5561 - acc: 0.7800
48/60 [=======================>......] - ETA: 0s - loss: 0.5575 - acc: 0.7792
51/60 [========================>.....] - ETA: 0s - loss: 0.5552 - acc: 0.7824
54/60 [==========================>...] - ETA: 0s - loss: 0.5547 - acc: 0.7778
56/60 [===========================>..] - ETA: 0s - loss: 0.5512 - acc: 0.7804
58/60 [============================>.] - ETA: 0s - loss: 0.5521 - acc: 0.7793
60/60 [==============================] - 3s 56ms/step - loss: 0.5494 - acc: 0.7817 - val_loss: 0.4132 - val_acc: 0.8562
2022-07-01 21:07:48,627 [INFO] root: Training loop in progress
Epoch 5/10

 1/60 [..............................] - ETA: 1s - loss: 0.5423 - acc: 0.8000
 3/60 [>.............................] - ETA: 1s - loss: 0.4893 - acc: 0.7667
 5/60 [=>............................] - ETA: 1s - loss: 0.5139 - acc: 0.7800
 7/60 [==>...........................] - ETA: 1s - loss: 0.5199 - acc: 0.7714
 9/60 [===>..........................] - ETA: 1s - loss: 0.4990 - acc: 0.7889
11/60 [====>.........................] - ETA: 1s - loss: 0.4750 - acc: 0.8000
14/60 [======>.......................] - ETA: 1s - loss: 0.4619 - acc: 0.8071
17/60 [=======>......................] - ETA: 1s - loss: 0.4657 - acc: 0.7941
20/60 [=========>....................] - ETA: 1s - loss: 0.5138 - acc: 0.7800
22/60 [==========>...................] - ETA: 0s - loss: 0.5153 - acc: 0.7818
23/60 [==========>...................] - ETA: 1s - loss: 0.5232 - acc: 0.7783
25/60 [===========>..................] - ETA: 1s - loss: 0.5209 - acc: 0.7840
26/60 [============>.................] - ETA: 1s - loss: 0.5241 - acc: 0.7846
29/60 [=============>................] - ETA: 0s - loss: 0.5169 - acc: 0.7897
30/60 [==============>...............] - ETA: 0s - loss: 0.5171 - acc: 0.7900
32/60 [===============>..............] - ETA: 0s - loss: 0.5083 - acc: 0.7969
34/60 [================>.............] - ETA: 0s - loss: 0.5069 - acc: 0.8000
37/60 [=================>............] - ETA: 0s - loss: 0.5201 - acc: 0.7946
38/60 [==================>...........] - ETA: 0s - loss: 0.5157 - acc: 0.8000
41/60 [===================>..........] - ETA: 0s - loss: 0.5120 - acc: 0.8000
42/60 [====================>.........] - ETA: 0s - loss: 0.5108 - acc: 0.8000
45/60 [=====================>........] - ETA: 0s - loss: 0.5090 - acc: 0.8044
48/60 [=======================>......] - ETA: 0s - loss: 0.4995 - acc: 0.8146
49/60 [=======================>......] - ETA: 0s - loss: 0.5109 - acc: 0.8082
50/60 [========================>.....] - ETA: 0s - loss: 0.5098 - acc: 0.8100
53/60 [=========================>....] - ETA: 0s - loss: 0.5039 - acc: 0.8094
55/60 [==========================>...] - ETA: 0s - loss: 0.5121 - acc: 0.8036
56/60 [===========================>..] - ETA: 0s - loss: 0.5098 - acc: 0.8036
59/60 [============================>.] - ETA: 0s - loss: 0.5111 - acc: 0.8034
60/60 [==============================] - 3s 55ms/step - loss: 0.5105 - acc: 0.8050 - val_loss: 0.3428 - val_acc: 0.8595
2022-07-01 21:07:52,934 [INFO] root: Training loop in progress
Epoch 6/10

 1/60 [..............................] - ETA: 1s - loss: 0.5872 - acc: 0.8000
 3/60 [>.............................] - ETA: 1s - loss: 0.4709 - acc: 0.8667
 5/60 [=>............................] - ETA: 1s - loss: 0.4184 - acc: 0.8800
 7/60 [==>...........................] - ETA: 1s - loss: 0.4761 - acc: 0.8571
10/60 [====>.........................] - ETA: 1s - loss: 0.5066 - acc: 0.8100
13/60 [=====>........................] - ETA: 1s - loss: 0.4847 - acc: 0.8231
16/60 [=======>......................] - ETA: 1s - loss: 0.4755 - acc: 0.8375
19/60 [========>.....................] - ETA: 1s - loss: 0.4873 - acc: 0.8263
20/60 [=========>....................] - ETA: 1s - loss: 0.4860 - acc: 0.8200
23/60 [==========>...................] - ETA: 1s - loss: 0.4704 - acc: 0.8304
26/60 [============>.................] - ETA: 1s - loss: 0.4775 - acc: 0.8192
27/60 [============>.................] - ETA: 1s - loss: 0.4748 - acc: 0.8185
30/60 [==============>...............] - ETA: 1s - loss: 0.4859 - acc: 0.8067
33/60 [===============>..............] - ETA: 0s - loss: 0.5019 - acc: 0.7939
35/60 [================>.............] - ETA: 0s - loss: 0.4879 - acc: 0.8057
36/60 [=================>............] - ETA: 0s - loss: 0.4896 - acc: 0.8028
39/60 [==================>...........] - ETA: 0s - loss: 0.4898 - acc: 0.8000
42/60 [====================>.........] - ETA: 0s - loss: 0.4903 - acc: 0.7976
44/60 [=====================>........] - ETA: 0s - loss: 0.4869 - acc: 0.8023
47/60 [======================>.......] - ETA: 0s - loss: 0.4889 - acc: 0.8000
49/60 [=======================>......] - ETA: 0s - loss: 0.4900 - acc: 0.7980
50/60 [========================>.....] - ETA: 0s - loss: 0.4893 - acc: 0.7980
53/60 [=========================>....] - ETA: 0s - loss: 0.4878 - acc: 0.8000
55/60 [==========================>...] - ETA: 0s - loss: 0.4901 - acc: 0.7982
57/60 [===========================>..] - ETA: 0s - loss: 0.4930 - acc: 0.7982
60/60 [==============================] - 3s 56ms/step - loss: 0.4854 - acc: 0.8017 - val_loss: 0.3362 - val_acc: 0.8660
2022-07-01 21:07:57,252 [INFO] root: Training loop in progress
Epoch 7/10

 1/60 [..............................] - ETA: 1s - loss: 0.5650 - acc: 0.8000
 3/60 [>.............................] - ETA: 1s - loss: 0.6966 - acc: 0.7333
 5/60 [=>............................] - ETA: 1s - loss: 0.6858 - acc: 0.7000
 7/60 [==>...........................] - ETA: 1s - loss: 0.6773 - acc: 0.7000
 9/60 [===>..........................] - ETA: 1s - loss: 0.5998 - acc: 0.7444
11/60 [====>.........................] - ETA: 1s - loss: 0.5591 - acc: 0.7727
13/60 [=====>........................] - ETA: 1s - loss: 0.5341 - acc: 0.7923
16/60 [=======>......................] - ETA: 1s - loss: 0.5147 - acc: 0.7937
17/60 [=======>......................] - ETA: 1s - loss: 0.5353 - acc: 0.7706
20/60 [=========>....................] - ETA: 1s - loss: 0.5035 - acc: 0.7950
21/60 [=========>....................] - ETA: 1s - loss: 0.4967 - acc: 0.7952
23/60 [==========>...................] - ETA: 1s - loss: 0.5090 - acc: 0.7913
26/60 [============>.................] - ETA: 1s - loss: 0.5086 - acc: 0.7846
28/60 [=============>................] - ETA: 1s - loss: 0.5026 - acc: 0.7857
30/60 [==============>...............] - ETA: 1s - loss: 0.4930 - acc: 0.7900
32/60 [===============>..............] - ETA: 0s - loss: 0.4796 - acc: 0.8000
34/60 [================>.............] - ETA: 0s - loss: 0.4778 - acc: 0.8000
35/60 [================>.............] - ETA: 0s - loss: 0.4752 - acc: 0.8000
36/60 [=================>............] - ETA: 0s - loss: 0.4743 - acc: 0.8028
39/60 [==================>...........] - ETA: 0s - loss: 0.4762 - acc: 0.8026
41/60 [===================>..........] - ETA: 0s - loss: 0.4672 - acc: 0.8073
43/60 [====================>.........] - ETA: 0s - loss: 0.4674 - acc: 0.8000
46/60 [======================>.......] - ETA: 0s - loss: 0.4657 - acc: 0.8022
47/60 [======================>.......] - ETA: 0s - loss: 0.4676 - acc: 0.8000
49/60 [=======================>......] - ETA: 0s - loss: 0.4679 - acc: 0.8000
51/60 [========================>.....] - ETA: 0s - loss: 0.4671 - acc: 0.8020
54/60 [==========================>...] - ETA: 0s - loss: 0.4709 - acc: 0.8037
55/60 [==========================>...] - ETA: 0s - loss: 0.4704 - acc: 0.8055
56/60 [===========================>..] - ETA: 0s - loss: 0.4703 - acc: 0.8071
57/60 [===========================>..] - ETA: 0s - loss: 0.4718 - acc: 0.8053
60/60 [==============================] - 3s 54ms/step - loss: 0.4712 - acc: 0.8033 - val_loss: 0.3064 - val_acc: 0.8627
2022-07-01 21:08:01,581 [INFO] root: Training loop in progress
Epoch 8/10

 1/60 [..............................] - ETA: 1s - loss: 0.3875 - acc: 0.8000
 3/60 [>.............................] - ETA: 1s - loss: 0.4006 - acc: 0.9000
 5/60 [=>............................] - ETA: 1s - loss: 0.3749 - acc: 0.9400
 6/60 [==>...........................] - ETA: 1s - loss: 0.3625 - acc: 0.9333
 8/60 [===>..........................] - ETA: 1s - loss: 0.4299 - acc: 0.8875
11/60 [====>.........................] - ETA: 1s - loss: 0.4738 - acc: 0.8455
13/60 [=====>........................] - ETA: 1s - loss: 0.4586 - acc: 0.8538
16/60 [=======>......................] - ETA: 1s - loss: 0.4798 - acc: 0.8313
17/60 [=======>......................] - ETA: 1s - loss: 0.4765 - acc: 0.8412
20/60 [=========>....................] - ETA: 1s - loss: 0.4993 - acc: 0.8300
21/60 [=========>....................] - ETA: 1s - loss: 0.4915 - acc: 0.8381
24/60 [===========>..................] - ETA: 1s - loss: 0.5009 - acc: 0.8292
25/60 [===========>..................] - ETA: 1s - loss: 0.4986 - acc: 0.8280
28/60 [=============>................] - ETA: 0s - loss: 0.4979 - acc: 0.8250
30/60 [==============>...............] - ETA: 0s - loss: 0.4918 - acc: 0.8233
32/60 [===============>..............] - ETA: 0s - loss: 0.4887 - acc: 0.8219
35/60 [================>.............] - ETA: 0s - loss: 0.4842 - acc: 0.8257
36/60 [=================>............] - ETA: 0s - loss: 0.4867 - acc: 0.8194
39/60 [==================>...........] - ETA: 0s - loss: 0.4949 - acc: 0.8179
42/60 [====================>.........] - ETA: 0s - loss: 0.4934 - acc: 0.8167
43/60 [====================>.........] - ETA: 0s - loss: 0.4915 - acc: 0.8163
46/60 [======================>.......] - ETA: 0s - loss: 0.4981 - acc: 0.8130
49/60 [=======================>......] - ETA: 0s - loss: 0.5065 - acc: 0.8061
50/60 [========================>.....] - ETA: 0s - loss: 0.5038 - acc: 0.8100
53/60 [=========================>....] - ETA: 0s - loss: 0.5003 - acc: 0.8094
56/60 [===========================>..] - ETA: 0s - loss: 0.4965 - acc: 0.8107
59/60 [============================>.] - ETA: 0s - loss: 0.4967 - acc: 0.8102
60/60 [==============================] - 3s 56ms/step - loss: 0.4962 - acc: 0.8100 - val_loss: 0.3204 - val_acc: 0.8791
2022-07-01 21:08:05,867 [INFO] root: Training loop in progress
Epoch 9/10

 1/60 [..............................] - ETA: 1s - loss: 0.2372 - acc: 1.0000
 3/60 [>.............................] - ETA: 1s - loss: 0.4902 - acc: 0.8333
 5/60 [=>............................] - ETA: 1s - loss: 0.4927 - acc: 0.8600
 8/60 [===>..........................] - ETA: 1s - loss: 0.4770 - acc: 0.8500
11/60 [====>.........................] - ETA: 1s - loss: 0.5029 - acc: 0.8182
13/60 [=====>........................] - ETA: 1s - loss: 0.4788 - acc: 0.8231
16/60 [=======>......................] - ETA: 1s - loss: 0.4534 - acc: 0.8312
19/60 [========>.....................] - ETA: 1s - loss: 0.4393 - acc: 0.8421
22/60 [==========>...................] - ETA: 1s - loss: 0.4289 - acc: 0.8455
25/60 [===========>..................] - ETA: 1s - loss: 0.4563 - acc: 0.8360
26/60 [============>.................] - ETA: 1s - loss: 0.4550 - acc: 0.8346
29/60 [=============>................] - ETA: 1s - loss: 0.4475 - acc: 0.8276
32/60 [===============>..............] - ETA: 0s - loss: 0.4633 - acc: 0.8219
34/60 [================>.............] - ETA: 0s - loss: 0.4673 - acc: 0.8147
37/60 [=================>............] - ETA: 0s - loss: 0.4706 - acc: 0.8135
39/60 [==================>...........] - ETA: 0s - loss: 0.4697 - acc: 0.8128
41/60 [===================>..........] - ETA: 0s - loss: 0.4727 - acc: 0.8098
44/60 [=====================>........] - ETA: 0s - loss: 0.4619 - acc: 0.8136
47/60 [======================>.......] - ETA: 0s - loss: 0.4569 - acc: 0.8191
50/60 [========================>.....] - ETA: 0s - loss: 0.4622 - acc: 0.8140
53/60 [=========================>....] - ETA: 0s - loss: 0.4578 - acc: 0.8208
55/60 [==========================>...] - ETA: 0s - loss: 0.4583 - acc: 0.8218
58/60 [============================>.] - ETA: 0s - loss: 0.4536 - acc: 0.8259
60/60 [==============================] - 3s 55ms/step - loss: 0.4614 - acc: 0.8200 - val_loss: 0.2929 - val_acc: 0.8922
2022-07-01 21:08:10,263 [INFO] root: Training loop in progress
Epoch 10/10

 1/60 [..............................] - ETA: 1s - loss: 0.5384 - acc: 0.7000
 3/60 [>.............................] - ETA: 1s - loss: 0.4378 - acc: 0.8667
 5/60 [=>............................] - ETA: 1s - loss: 0.5017 - acc: 0.8200
 7/60 [==>...........................] - ETA: 1s - loss: 0.4755 - acc: 0.8286
 9/60 [===>..........................] - ETA: 1s - loss: 0.4815 - acc: 0.8333
12/60 [=====>........................] - ETA: 1s - loss: 0.5225 - acc: 0.7917
15/60 [======>.......................] - ETA: 1s - loss: 0.5152 - acc: 0.8000
17/60 [=======>......................] - ETA: 1s - loss: 0.5306 - acc: 0.7941
20/60 [=========>....................] - ETA: 1s - loss: 0.4991 - acc: 0.8050
22/60 [==========>...................] - ETA: 1s - loss: 0.4979 - acc: 0.8091
23/60 [==========>...................] - ETA: 1s - loss: 0.4918 - acc: 0.8130
25/60 [===========>..................] - ETA: 1s - loss: 0.4811 - acc: 0.8200
27/60 [============>.................] - ETA: 1s - loss: 0.4751 - acc: 0.8222
28/60 [=============>................] - ETA: 1s - loss: 0.4813 - acc: 0.8143
29/60 [=============>................] - ETA: 1s - loss: 0.4801 - acc: 0.8138
32/60 [===============>..............] - ETA: 1s - loss: 0.4834 - acc: 0.8094
34/60 [================>.............] - ETA: 0s - loss: 0.4781 - acc: 0.8088
35/60 [================>.............] - ETA: 0s - loss: 0.4739 - acc: 0.8114
38/60 [==================>...........] - ETA: 0s - loss: 0.4753 - acc: 0.8158
41/60 [===================>..........] - ETA: 0s - loss: 0.4678 - acc: 0.8195
42/60 [====================>.........] - ETA: 0s - loss: 0.4682 - acc: 0.8190
45/60 [=====================>........] - ETA: 0s - loss: 0.4688 - acc: 0.8178
46/60 [======================>.......] - ETA: 0s - loss: 0.4640 - acc: 0.8217
49/60 [=======================>......] - ETA: 0s - loss: 0.4622 - acc: 0.8245
52/60 [=========================>....] - ETA: 0s - loss: 0.4615 - acc: 0.8269
54/60 [==========================>...] - ETA: 0s - loss: 0.4575 - acc: 0.8296
56/60 [===========================>..] - ETA: 0s - loss: 0.4616 - acc: 0.8286
58/60 [============================>.] - ETA: 0s - loss: 0.4545 - acc: 0.8345
60/60 [==============================] - 3s 55ms/step - loss: 0.4560 - acc: 0.8333 - val_loss: 0.2956 - val_acc: 0.8856
2022-07-01 21:08:14,538 [INFO] root: Training loop in progress
2022-07-01 21:08:14,539 [INFO] root: Training loop complete.
2022-07-01 21:08:14,539 [INFO] root: Final model evaluation in progress.
2022-07-01 21:08:17,175 [INFO] root: Model evaluation in complete.
2022-07-01 21:08:17,176 [INFO] __main__: Total Val Loss: 0.2955627739429474
2022-07-01 21:08:17,176 [INFO] __main__: Total Val accuracy: 0.8856208920478821
2022-07-01 21:08:17,176 [INFO] root: Training finished successfully.
2022-07-01 21:08:17,176 [INFO] __main__: Training finished successfully.